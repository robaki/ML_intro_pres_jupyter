{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a build-in datatset from sklearn\n",
    "# contains data about iris flowers\n",
    "# the task is to learn how to recognise the type/specie? of flower based on dimensions\n",
    "iris = datasets.load_iris()\n",
    "columns = iris['feature_names']\n",
    "iris_df = pd.DataFrame(iris.data, columns=columns)\n",
    "iris_df['class'] = pd.Series(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using train_test_split to split data randomly into train and test sets\n",
    "# X is an array containing features, y contains classes / lables (types of flowers)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_df[columns], iris_df['class'], stratify=iris_df['class'], test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of a pipe - a sequence of preprocessors or/and models\n",
    "# here the first object is MinMaxScaler, the second KNN classifier\n",
    "# Strings are the names of these objects, and will be used to pass hyperparameters to them\n",
    "estimators = [('Scaler', preprocessing.MinMaxScaler()), ('clf', KNeighborsClassifier())]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# to find the best set of hyperparameters we need to first define what hyperparameters to check\n",
    "# and what objects should use them. The keys start with object name (clf or Scaler in this case)\n",
    "# that we defined above, followed by a __ separator and the name of a hyperparameter (keyword)\n",
    "# values are lists of alternative values.\n",
    "# In the case of feature_range (passed to the Scaler) there is only one value (0,1)\n",
    "# In the case of n_neighbours (passed to the clf) there are 4 values to check: 5, 10, 25, 50\n",
    "param_grid = {\n",
    "    'Scaler__feature_range':[(0,1)],\n",
    "    'clf__n_neighbors':[5, 10, 25, 50], \n",
    "    'clf__weights':['uniform', 'distance'], \n",
    "    'clf__metric':['minkowski', 'manhattan', 'chebyshev'],\n",
    "}\n",
    "\n",
    "# parameter grid and pipe are passed to GridSearchCV - Grid Search CrossValidator\n",
    "# we also pass a name of a scoring method\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, scoring='f1_micro')\n",
    "\n",
    "# GridSearchCV generates all combinations of parameters from the param_grid,\n",
    "# applies them to the pipe (scaler and classifier)\n",
    "# and performs crossvalidation to check which parameter set is the best\n",
    "# according to the scoring method that was given above\n",
    "#\n",
    "# using pipe and grid search (apart from being convinient) solves one problem with \n",
    "# manual preprocessing and crossvalidating using cross_validate():\n",
    "# scaler should not be fitted on the whole dataset used for crossvalidation, but only\n",
    "# on the random subset of crossvalidation dataset that is used for training - \n",
    "# so each round of crossvalidation should fit scaler separately, leaving out the validation subset\n",
    "#\n",
    "# in grid_search it is done the second way \n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get best parameter set \n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new scaler and fitting it on all training data\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# get the best hyperparameters as a dict with correct names (keys)\n",
    "# (without pipe objects names)\n",
    "kwargs_clf = {key.split('__')[1] : val for key, val in grid_search.best_params_.items() if key.startswith('clf__')}\n",
    "\n",
    "# create a classifier using the best hyperparameters, train and apply to test data\n",
    "clf = KNeighborsClassifier(**kwargs_clf)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# check results\n",
    "metrics.confusion_matrix(y_test, y_pred, labels=[0, 1, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
